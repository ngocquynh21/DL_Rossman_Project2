{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1953f3d7-3910-4d74-8c39-5c8728007ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stores in Train: 1115\n",
      "Unique stores in Store: 1115\n",
      "Unique stores in Test: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['StateHoliday'] = df['StateHoliday'].astype(str).replace({'0':0, 'a':1, 'b':2, 'c':3}).astype(int)\n",
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['StoreType'] = df['StoreType'].replace({'a':1, 'b':2, 'c':3, 'd':4}).astype(int)\n",
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:80: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Assortment'] = df['Assortment'].replace({'a':1, 'b':2, 'c':3}).astype(int)\n",
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:74: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['StateHoliday'] = df['StateHoliday'].astype(str).replace({'0':0, 'a':1, 'b':2, 'c':3}).astype(int)\n",
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['StoreType'] = df['StoreType'].replace({'a':1, 'b':2, 'c':3, 'd':4}).astype(int)\n",
      "C:\\Users\\tp25215\\AppData\\Local\\Temp\\ipykernel_13672\\3640560257.py:80: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Assortment'] = df['Assortment'].replace({'a':1, 'b':2, 'c':3}).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 3.2391 - val_loss: 0.0559\n",
      "Epoch 2/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0394\n",
      "Epoch 3/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0251 - val_loss: 0.0355\n",
      "Epoch 4/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0195 - val_loss: 0.0338\n",
      "Epoch 5/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0172 - val_loss: 0.0339\n",
      "Epoch 6/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0160 - val_loss: 0.0325\n",
      "Epoch 7/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0153 - val_loss: 0.0311\n",
      "Epoch 8/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0149 - val_loss: 0.0317\n",
      "Epoch 9/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0146 - val_loss: 0.0363\n",
      "Epoch 10/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0145 - val_loss: 0.0371\n",
      "Epoch 11/20\n",
      "\u001b[1m2969/2969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0142 - val_loss: 0.0360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x185990e7980>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Read data by pandas \n",
    "train = pd.read_csv('train.csv', parse_dates=['Date'], low_memory=False) \n",
    "test = pd.read_csv('test.csv', parse_dates=['Date'], low_memory=False)\n",
    "store = pd.read_csv('store.csv', low_memory=False)\n",
    "#train.count()\n",
    "#train.head()\n",
    "#test.head()\n",
    "#test.count()\n",
    "#store.head()\n",
    "#store.count()\n",
    "\n",
    "# 2. Check data to be sure that all the stores in test and train set have information in store.csv\n",
    "print(\"Unique stores in Train:\", train['Store'].nunique())\n",
    "print(\"Unique stores in Store:\", store['Store'].nunique())\n",
    "print(\"Unique stores in Test:\", test['Store'].nunique())\n",
    "# T_train = set(pd.read_csv('train.csv', usecols=['Store'])['Store'].unique())\n",
    "# T_test = set(pd.read_csv('test.csv', usecols=['Store'])['Store'].unique())\n",
    "# T_store = set(pd.read_csv('store.csv', usecols=['Store'])['Store'].unique())\n",
    "# is_train_equal_store = T_train.issubset(T_store) and T_store.issubset(T_train)\n",
    "# is_test_in_train = T_test.issubset(T_train)\n",
    "# is_test_in_store = T_test.issubset(T_store)\n",
    "\n",
    "\n",
    "#3. Merge data, get the store information from store.csv into train and test set\n",
    "train = pd.merge(train, store, on='Store', how='left')\n",
    "test = pd.merge(test, store, on='Store', how='left')\n",
    "\n",
    "#4. Handle with Missing Values\n",
    "def handle_missing_values(df):\n",
    "    df['CompetitionDistance'] = df['CompetitionDistance'].fillna(df['CompetitionDistance'].median())\n",
    "    df['CompetitionOpenSinceMonth'] = df['CompetitionOpenSinceMonth'].fillna(0)\n",
    "    df['CompetitionOpenSinceYear'] = df['CompetitionOpenSinceYear'].fillna(0)\n",
    "    df['Promo2SinceWeek'] = df['Promo2SinceWeek'].fillna(0)\n",
    "    df['Promo2SinceYear'] = df['Promo2SinceYear'].fillna(0)\n",
    "    df['PromoInterval'] = df['PromoInterval'].fillna('0')\n",
    "    if 'Open' in df.columns:\n",
    "        df['Open'] = df['Open'].fillna(1)\n",
    "    return df\n",
    "# call instance\n",
    "train = handle_missing_values(train)\n",
    "test = handle_missing_values(test)\n",
    "\n",
    "# handling with noise \n",
    "train = train[(train[\"Open\"] != 0) & (train[\"Sales\"] > 0)].copy()\n",
    "y_train = np.log1p(train['Sales'])\n",
    "test_ids = test['Id']\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineer_safe(df):\n",
    "    # time\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # Competition Open Time\n",
    "    df['CompetitionOpen'] = 12 * (df['Year'] - df['CompetitionOpenSinceYear']) + \\\n",
    "        (df['Month'] - df['CompetitionOpenSinceMonth']) #How many months has the competitor been open?\"\n",
    "    df['CompetitionOpen'] = df['CompetitionOpen'].apply(lambda x: x if x > 0 else 0) \n",
    "    #If the result < 0 (ie the opponent has not opened at that time), set it to 0\n",
    "    # --- FIXED ENCODING (MANUAL MAPPING) ---\n",
    "    # Instead of letting the machine number itself, I  stipulate: a->1, b->2...\n",
    "    \n",
    "    # 1. StateHoliday\n",
    "    df['StateHoliday'] = df['StateHoliday'].astype(str).replace({'0':0, 'a':1, 'b':2, 'c':3}).astype(int)\n",
    "    \n",
    "    # 2. StoreType\n",
    "    df['StoreType'] = df['StoreType'].replace({'a':1, 'b':2, 'c':3, 'd':4}).astype(int)\n",
    "    \n",
    "    # 3. Assortment\n",
    "    df['Assortment'] = df['Assortment'].replace({'a':1, 'b':2, 'c':3}).astype(int)\n",
    "    \n",
    "    # 4. PromoInterval\n",
    "    intervals = {'0':0, 'Jan,Apr,Jul,Oct':1, 'Feb,May,Aug,Nov':2, 'Mar,Jun,Sep,Dec':3}\n",
    "    # Use map to be safe, if there is a strange value it will become NaN -> fillna(0)\n",
    "    df['PromoInterval'] = df['PromoInterval'].map(intervals).fillna(0).astype(int)\n",
    "\n",
    "    #5. Store ID: Subtract 1 to run from 0 -> 1114 (suitable for Embedding's index)\n",
    "    df['Store'] = df['Store'] - 1\n",
    "    \n",
    "    # 6. Year: Map year 2013->0, 2014->1, 2015->2\n",
    "    year_map = {2013:0, 2014:1, 2015:2}\n",
    "    df['Year'] = df['Year'].map(year_map).fillna(0).astype(int)\n",
    "    \n",
    "    # 7. Month: minus 1 to run from  0 -> 11\n",
    "    df['Month'] = df['Month'] - 1\n",
    "    \n",
    "    #8. WeekOfYear: Subtract 1 to run from 0 -> 51\n",
    "    df['WeekOfYear'] = df['WeekOfYear'] - 1\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer_safe(train)\n",
    "test = feature_engineer_safe(test)\n",
    "\n",
    "# --- PREPARATION OF INPUT FOR MODEL ---\n",
    "cat_features = ['Store', 'DayOfWeek', 'Month', 'Year', 'WeekOfYear', 'StoreType', \n",
    "                'Assortment', 'PromoInterval', 'StateHoliday']\n",
    "\n",
    "cont_features = ['CompetitionDistance', 'CompetitionOpen', 'Promo', 'SchoolHoliday', \n",
    "                 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'Day'] # fill out Open because Train = 1\n",
    "\n",
    "# scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_cont = scaler.fit_transform(train[cont_features]) #dealing with the problem of difference between 1 year vs 75000m\n",
    "X_test_cont = scaler.transform(test[cont_features])\n",
    "\n",
    "# --- Build MODEL (EMBEDDING) ---\n",
    "\n",
    "inputs = []\n",
    "embeddings = []\n",
    "\n",
    "vocab_map = {\n",
    "    'Store': 1115,\n",
    "    'DayOfWeek': 7,\n",
    "    'Month': 12,\n",
    "    'Year': 3,\n",
    "    'WeekOfYear': 53,\n",
    "    'StoreType': 5,     # 1-4, add 1 \n",
    "    'Assortment': 4,    # 1-3\n",
    "    'PromoInterval': 4, # 0-3\n",
    "    'StateHoliday': 4   # 0-3\n",
    "}\n",
    "#Keras TensorFlow\n",
    "for col in cat_features:\n",
    "    inp = Input(shape=(1,), name=f'in_{col}')\n",
    "    inputs.append(inp)\n",
    "    # Dim = min(50, size/2)~ Number of characteristics using half the value (but the maximum level is not more than 50).\n",
    "    dim = min(50, (vocab_map[col]+1)//2) \n",
    "    emb = Embedding(vocab_map[col], dim, name=f'emb_{col}')(inp)\n",
    "    embeddings.append(Flatten()(emb))\n",
    "\n",
    "# Continuous Input\n",
    "inp_cont = Input(shape=(len(cont_features),), name='in_cont')\n",
    "inputs.append(inp_cont)\n",
    "\n",
    "# concate\n",
    "x = concatenate(embeddings + [inp_cont])\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Hidden Layers\n",
    "x = Dense(256, activation='relu')(x) #256 neurons\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x) #128 neurons\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "output = Dense(1, activation='linear')(x) #1 nreuron sales\n",
    "\n",
    "model = Model(inputs, output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse') #Adaptive Moment Estimation\n",
    "\n",
    "# --- TRAIN ---\n",
    "X_train_list = [train[col].values for col in cat_features] + [X_train_cont]\n",
    "X_test_list = [test[col].values for col in cat_features] + [X_test_cont]\n",
    "\n",
    "model.fit(X_train_list, y_train, \n",
    "          validation_split=0.1, # 10% for validation\n",
    "          epochs=20, \n",
    "          batch_size=256, # 256 records/turn\n",
    "          callbacks=[EarlyStopping(patience=4, restore_best_weights=True)]) #if after 4 epochs the val_loss doesn't improve->stop\n",
    "# 1 epoch: 2969 turns, 256 each turn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4965c0bd-cec7-481e-aac3-c4f3d670d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1284/1284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 900us/step\n"
     ]
    }
   ],
   "source": [
    "# --- Predict ---\n",
    "preds_log = model.predict(X_test_list).flatten() # (41088, 1) (41088,) array\n",
    "preds = np.expm1(preds_log) #e^x - 1\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ids, 'Sales': preds})\n",
    "\n",
    "# Handle closure: Retrieve original Open information from test file\n",
    "test_origin = pd.read_csv('test.csv', usecols=['Id', 'Open'])\n",
    "# Fill NaN Open = 1 \n",
    "test_origin['Open'] = test_origin['Open'].fillna(1)\n",
    "# Handling closure: Retrieve information # Assign Sales = 0 for closed days Open original from test file\n",
    "submission.loc[test_origin['Open'] == 0, 'Sales'] = 0\n",
    "\n",
    "submission['Sales'] = submission['Sales'].apply(lambda x: max(0, x))\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83527b3-45e5-4f1e-bb3f-ffcecee2d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
